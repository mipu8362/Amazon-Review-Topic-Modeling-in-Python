{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling Amazon Product and Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling Amazon Product and Review Data\n",
    "# Open product file.\n",
    "count = 0\n",
    " \n",
    "loadedjson = open('meta_Clothing_Shoes_and_Jewelry.json', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "allproducts = {}\n",
    " \n",
    "listofcategories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7611"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for aline in loadedjson:\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print(count)\n",
    "    aproduct = eval(aline)\n",
    "    \n",
    "    allproducts[aproduct['asin']] = aproduct\n",
    "    \n",
    "    for categories in aproduct['categories']:\n",
    "        for acategory in categories:\n",
    "            if acategory in listofcategories:\n",
    "                listofcategories[acategory] += 1\n",
    "            if acategory not in listofcategories:\n",
    "                listofcategories[acategory] = 1                \n",
    "  \n",
    "listofcategories['Puma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "allpumaasins = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006651660520532346\n",
      "0.013303321041064691\n",
      "0.019954981561597036\n",
      "0.026606642082129383\n",
      "0.03325830260266173\n",
      "0.03990996312319407\n",
      "0.04656162364372642\n",
      "0.053213284164258766\n",
      "0.05986494468479111\n",
      "0.06651660520532346\n",
      "0.0731682657258558\n",
      "0.07981992624638815\n",
      "0.08647158676692049\n",
      "0.09312324728745285\n",
      "0.09977490780798519\n",
      "0.10642656832851753\n",
      "0.11307822884904987\n",
      "0.11972988936958222\n",
      "0.12638154989011458\n",
      "0.13303321041064692\n",
      "0.13968487093117926\n",
      "0.1463365314517116\n",
      "0.15298819197224395\n",
      "0.1596398524927763\n",
      "0.16629151301330863\n",
      "0.17294317353384098\n",
      "0.17959483405437332\n",
      "0.1862464945749057\n",
      "0.19289815509543803\n",
      "0.19954981561597038\n",
      "0.20620147613650272\n",
      "0.21285313665703506\n",
      "0.2195047971775674\n",
      "0.22615645769809975\n",
      "0.2328081182186321\n",
      "0.23945977873916444\n",
      "0.24611143925969678\n",
      "0.25276309978022915\n",
      "0.2594147603007615\n",
      "0.26606642082129384\n",
      "0.2727180813418262\n",
      "0.2793697418623585\n",
      "0.28602140238289087\n",
      "0.2926730629034232\n",
      "0.29932472342395555\n",
      "0.3059763839444879\n",
      "0.31262804446502024\n",
      "0.3192797049855526\n",
      "0.3259313655060849\n",
      "0.33258302602661727\n",
      "0.3392346865471496\n",
      "0.34588634706768195\n",
      "0.3525380075882143\n",
      "0.35918966810874664\n",
      "0.36584132862927904\n",
      "0.3724929891498114\n",
      "0.3791446496703437\n",
      "0.38579631019087607\n",
      "0.3924479707114084\n",
      "0.39909963123194075\n",
      "0.4057512917524731\n",
      "0.41240295227300544\n",
      "0.4190546127935378\n",
      "0.4257062733140701\n",
      "0.43235793383460247\n",
      "0.4390095943551348\n",
      "0.44566125487566716\n",
      "0.4523129153961995\n",
      "0.45896457591673184\n",
      "0.4656162364372642\n",
      "0.47226789695779653\n",
      "0.4789195574783289\n",
      "0.4855712179988612\n",
      "0.49222287851939356\n",
      "0.4988745390399259\n",
      "0.5055261995604583\n",
      "0.5121778600809906\n",
      "0.518829520601523\n",
      "0.5254811811220553\n",
      "0.5321328416425877\n",
      "0.53878450216312\n",
      "0.5454361626836524\n",
      "0.5520878232041847\n",
      "0.558739483724717\n",
      "0.5653911442452494\n",
      "0.5720428047657817\n",
      "0.5786944652863141\n",
      "0.5853461258068464\n",
      "0.5919977863273788\n",
      "0.5986494468479111\n",
      "0.6053011073684434\n",
      "0.6119527678889758\n",
      "0.6186044284095081\n",
      "0.6252560889300405\n",
      "0.6319077494505728\n",
      "0.6385594099711052\n",
      "0.6452110704916375\n",
      "0.6518627310121698\n",
      "0.6585143915327022\n",
      "0.6651660520532345\n",
      "0.6718177125737669\n",
      "0.6784693730942992\n",
      "0.6851210336148316\n",
      "0.6917726941353639\n",
      "0.6984243546558963\n",
      "0.7050760151764286\n",
      "0.7117276756969609\n",
      "0.7183793362174933\n",
      "0.7250309967380257\n",
      "0.7316826572585581\n",
      "0.7383343177790904\n",
      "0.7449859782996228\n",
      "0.7516376388201551\n",
      "0.7582892993406875\n",
      "0.7649409598612198\n",
      "0.7715926203817521\n",
      "0.7782442809022845\n",
      "0.7848959414228168\n",
      "0.7915476019433492\n",
      "0.7981992624638815\n",
      "0.8048509229844139\n",
      "0.8115025835049462\n",
      "0.8181542440254785\n",
      "0.8248059045460109\n",
      "0.8314575650665432\n",
      "0.8381092255870756\n",
      "0.8447608861076079\n",
      "0.8514125466281403\n",
      "0.8580642071486726\n",
      "0.8647158676692049\n",
      "0.8713675281897373\n",
      "0.8780191887102696\n",
      "0.884670849230802\n",
      "0.8913225097513343\n",
      "0.8979741702718667\n",
      "0.904625830792399\n",
      "0.9112774913129313\n",
      "0.9179291518334637\n",
      "0.924580812353996\n",
      "0.9312324728745284\n",
      "0.9378841333950607\n",
      "0.9445357939155931\n",
      "0.9511874544361254\n",
      "0.9578391149566577\n",
      "0.9644907754771901\n",
      "0.9711424359977224\n",
      "0.9777940965182548\n",
      "0.9844457570387871\n",
      "0.9910974175593195\n",
      "0.9977490780798518\n"
     ]
    }
   ],
   "source": [
    "for aproduct in allproducts:\n",
    "   theproduct = allproducts[aproduct]\n",
    "   count += 1\n",
    "   if count % 10000 == 0:\n",
    "        print(count/1503384)\n",
    "   for categories in theproduct['categories']:\n",
    "        for acategory in categories:\n",
    "            if 'puma' in acategory.lower():\n",
    "                allpumaasins.add(theproduct['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Puma ASINs file.\n",
    "file = open('Michelle_Puglio-ASINS_puma.txt', 'w')\n",
    "file.write(','.join(allpumaasins))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open review file.\n",
    "loadedjson = open('reviews_Clothing_Shoes_and_Jewelry.json', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "allreviews = {}\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-bd73d2754a53>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    areview = eval(aline)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    {\"reviewerID\": \"A2YBPW\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "for aline in loadedjson:\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print(count)\n",
    "    areview = eval(aline)\n",
    "    theasin = areview['asin']\n",
    "    thereviewer = areview['reviewerID']\n",
    "    \n",
    "    if theasin in allpumaasins:\n",
    "        thekey = '%s.%s' % (theasin,thereviewer)\n",
    "        allreviews[thekey] = areview\n",
    "        \n",
    "len(allreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a JSON file of the Amazon Puma review data you'll use for topic \n",
    "# modeling.\n",
    "import json\n",
    "json.dump(allreviews, open('allpumareviews.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means Topic Modeling (Worst Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_multiprocessing_helpers.py:38: UserWarning: [Errno 12] Cannot allocate memory.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading texts\n",
      "Top terms per cluster\n",
      "0: one star small baby\n",
      "1: apart shoes shoe fell\n",
      "2: money waste time shoes\n",
      "3: wrong size ordered shoes\n",
      "4: shoe 34 good size\n",
      "5: watch time working water\n",
      "6: quality poor bad price\n",
      "7: tight send front shoes\n",
      "8: size product large item\n",
      "9: pair foot shoe shoes\n",
      "10: shoes buy running bad\n",
      "11: way small sizing size\n",
      "12: good cheap color looks\n",
      "13: size shoes small fit\n",
      "14: narrow feet shoes shoe\n",
      "15: small size half runs\n",
      "16: uncomfortable shoe shoes feet\n",
      "17: big size change thank\n",
      "18: leather like made shoes\n",
      "19: dont shirt sole buy\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words.append('puma')\n",
    "\n",
    "\n",
    "texts = set()\n",
    "def load_texts(topicdata):\n",
    "    for areview in topicdata:\n",
    "        if 'reviewText' in topicdata[areview]:\n",
    "            if int(topicdata[areview]['overall']) <= 1:\n",
    "                reviewtext = topicdata[areview]['reviewText']\n",
    "                summary = topicdata[areview]['summary']\n",
    "                asin = topicdata[areview]['asin']\n",
    "                review = '%s %s %s' % (asin, summary, reviewtext)\n",
    "                texts.add(review)\n",
    "\n",
    "\n",
    "print('loading texts')\n",
    "load_texts(allreviews)\n",
    "\n",
    "documents = list(texts)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 20\n",
    " \n",
    "model = KMeans(n_clusters=true_k, max_iter=100000)\n",
    "model.fit(X)\n",
    "\n",
    "print('Top terms per cluster')\n",
    "\n",
    "\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' % (i, ' '.join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a .txt file to save printed topics from your working Topic Model.\n",
    "printed_topics = []\n",
    "count = 0\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    topic = '%d: %s' % (i, ' '.join(topic_terms))\n",
    "    if topic not in printed_topics:\n",
    "        printed_topics.append(topic)\n",
    "\n",
    "file = open('1_Star_Printed_Topics.txt', 'w')\n",
    "file.write('\\n'.join(printed_topics))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists!\n"
     ]
    }
   ],
   "source": [
    "# Gensim in Python\n",
    "import os\n",
    "outfiles = {}\n",
    "\n",
    "try:\n",
    "    os.mkdir('output')\n",
    "    \n",
    "except OSError:\n",
    "    print('Directory already exists!')\n",
    "    \n",
    "else:\n",
    "     print ('Successfully created the directory.')\n",
    "    \n",
    "\n",
    "for atopic in range(true_k):\n",
    "    topicterms = [terms[ind] for ind in order_centroids[atopic, :4]]\n",
    "    outfiles[atopic] = open(os.path.join('output', '_'.join(topicterms) + '.txt'), 'w')\n",
    "\n",
    "for areview in allreviews:\n",
    "    if 'reviewText' in allreviews[areview]:\n",
    "        areview = allreviews[areview]\n",
    "        reviewwithmetadata = \"%s %s %s\" % (areview['asin'], areview['summary'], areview['reviewText'])\n",
    "        Y = vectorizer.transform([reviewwithmetadata])\n",
    "        \n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % reviewwithmetadata)\n",
    "\n",
    "for n, f in outfiles.items():\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means Topic Modeling (Best Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading texts\n",
      "Top terms per cluster\n",
      "0: sneakers great comfortable love\n",
      "1: great comfortable well fit\n",
      "2: good quality product shoes\n",
      "3: like shoes fit really\n",
      "4: pair shoes another comfortable\n",
      "5: shoe great comfortable good\n",
      "6: bag gym b005lce58a great\n",
      "7: nice shoes really comfortable\n",
      "8: size small half shoes\n",
      "9: stars five four great\n",
      "10: loves son shoes old\n",
      "11: love shoes color fit\n",
      "12: watch great nice good\n",
      "13: cute super daughter shoes\n",
      "14: golf shoes shoe great\n",
      "15: great shoes fit price\n",
      "16: shoes comfortable great love\n",
      "17: feet shoes narrow wide\n",
      "18: running shoes great comfortable\n",
      "19: muy de el la\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words.append('puma')\n",
    "\n",
    "\n",
    "texts = set()\n",
    "def load_texts(topicdata):\n",
    "    for areview in topicdata:\n",
    "        if 'reviewText' in topicdata[areview]:\n",
    "            if int(topicdata[areview]['overall']) >= 4:\n",
    "                reviewtext = topicdata[areview]['reviewText']\n",
    "                summary = topicdata[areview]['summary']\n",
    "                asin = topicdata[areview]['asin']\n",
    "                review = '%s %s %s' % (asin, summary, reviewtext)\n",
    "                texts.add(review)\n",
    "\n",
    "\n",
    "print('loading texts')\n",
    "load_texts(allreviews)\n",
    "\n",
    "documents = list(texts)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 20\n",
    " \n",
    "model = KMeans(n_clusters=true_k, max_iter=100000)\n",
    "model.fit(X)\n",
    "\n",
    "print('Top terms per cluster')\n",
    "\n",
    "\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' % (i, ' '.join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a .txt file to save printed topics from your working Topic Model.\n",
    "printed_topics = []\n",
    "count = 0\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    topic = '%d: %s' % (i, ' '.join(topic_terms))\n",
    "    if topic not in printed_topics:\n",
    "        printed_topics.append(topic)\n",
    "\n",
    "file = open('4_Star_Printed_Topics.txt', 'w')\n",
    "file.write('\\n'.join(printed_topics))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists!\n"
     ]
    }
   ],
   "source": [
    "# Gensim in Python\n",
    "import os\n",
    "outfiles = {}\n",
    "\n",
    "try:\n",
    "    os.mkdir('output')\n",
    "    \n",
    "except OSError:\n",
    "    print('Directory already exists!')\n",
    "    \n",
    "else:\n",
    "     print ('Successfully created the directory.')\n",
    "    \n",
    "\n",
    "for atopic in range(true_k):\n",
    "    topicterms = [terms[ind] for ind in order_centroids[atopic, :4]]\n",
    "    outfiles[atopic] = open(os.path.join('output', '_'.join(topicterms) + '.txt'), 'w')\n",
    "\n",
    "for areview in allreviews:\n",
    "    if 'reviewText' in allreviews[areview]:\n",
    "        areview = allreviews[areview]\n",
    "        reviewwithmetadata = \"%s %s %s\" % (areview['asin'], areview['summary'], areview['reviewText'])\n",
    "        Y = vectorizer.transform([reviewwithmetadata])\n",
    "        \n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % reviewwithmetadata)\n",
    "\n",
    "for n, f in outfiles.items():\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
